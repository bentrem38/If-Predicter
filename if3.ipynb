{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoModelForSeq2SeqLM\n",
    "from transformers import RobertaTokenizer\n",
    "from datasets import DatasetDict\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import black\n",
    "import os\n",
    "import re\n",
    "from datasets import Dataset\n",
    "import autopep8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# 1. Install Required Libraries\n",
    "# ------------------------\n",
    "#!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "#!pip install transformers datasets evaluate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://huggingface.co/Salesforce/codet5-small\n",
    "# ------------------------------------------------------------------------\n",
    "# 2. Load Dataset (CodeXGLUE - Code Translation Java <=> C#)\n",
    "# ------------------------------------------------------------------------\n",
    "data_dir = r\"C:\\Users\\bentr\\Downloads\\Archive\\Archive\"\n",
    "\n",
    "# CodeXGLUE is a benchmark dataset collection by Microsoft for code-related tasks.\n",
    "# Here, we use the code-translation-python-java dataset.\n",
    "csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "# Read the CSV files into DataFrames\n",
    "test_dataset = load_dataset('csv', data_files=os.path.join(data_dir, csv_files[0]))['train']\n",
    "train_dataset = load_dataset('csv', data_files=os.path.join(data_dir, csv_files[1]))['train']\n",
    "validation_dataset = load_dataset('csv', data_files=os.path.join(data_dir, csv_files[2]))['train']\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'test': test_dataset,\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32101, 512)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = \"Salesforce/codet5-small\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.add_tokens([\"<MASK>\"]) #Imagine we need an extra token. This line adds the extra token to the vocabulary\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_method = dataset[\"train\"][\"cleaned_method\"][2]\n",
    "\n",
    "# Flatten the method by joining words with a single space\n",
    "flattened_method = \" \".join(flattened_method.split())\n",
    "flattened_method =re.sub(r'\\s*([=+\\-*/%<>!&|^(),:{}\\[\\].])\\s*', r'\\1', flattened_method)\n",
    "# Get and flatte n the target block\n",
    "target = dataset[\"train\"][\"target_block\"][2]\n",
    "target = re.sub(r'\\s*([=+\\-*/%<>!&|^(),:{}\\[\\].])\\s*', r'\\1', target)\n",
    "\n",
    "print(\"Target after normalization:\", target)\n",
    "\n",
    "# Replace target with <MASK> in the flattened method\n",
    "flattened_method = flattened_method.replace(target, \"<MASK>\")\n",
    "\n",
    "# Output the result\n",
    "print(\"Flattened method after replacemooent:\", flattened_method)\n",
    "print(\"Target:\", target)  # Shows the target for reference\n",
    "#\".egg\" + path . sep in filename :  #if \".egg\" + path . sep in filename:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def mask_dataset(dataset):\n",
    "    # Create lists to store processed methods and targets\n",
    "    processed_methods = []\n",
    "    processed_targets = []\n",
    "    i = 0\n",
    "\n",
    "    # Loop through the dataset and apply processing\n",
    "    yes = 0\n",
    "    no = 0\n",
    "    while i < 1000:\n",
    "        # Get the current method and target block\n",
    "        i += 1\n",
    "        flattened_method = dataset[\"train\"][\"cleaned_method\"][i]\n",
    "        target = dataset[\"train\"][\"target_block\"][i]\n",
    "\n",
    "        # Flatten the method by joining words with a single space\n",
    "        flattened_method = \" \".join(flattened_method.split())\n",
    "        flattened_method = re.sub(r'\\s*([=+\\-*/%<>!&|^(),:{}\\[\\].])\\s*', r'\\1', flattened_method)\n",
    "\n",
    "        # Normalize the target block\n",
    "        target = re.sub(r'\\s*([=+\\-*/%<>!&|^(),:{}\\[\\].])\\s*', r'\\1', target)\n",
    "\n",
    "        # Replace target with <MASK> in the flattened method\n",
    "        if target not in flattened_method:\n",
    "            no+=1\n",
    "        if target in flattened_method:\n",
    "            flattened_method = flattened_method.replace(target, \"<MASK>\")\n",
    "            yes+=1\n",
    "        # Append processed results\n",
    "        processed_methods.append(flattened_method)\n",
    "        processed_targets.append(target)\n",
    "    print(yes)\n",
    "    print(no)\n",
    "    # Build Dataset (not DatasetDict)\n",
    "    processed = Dataset.from_dict({\n",
    "        'processed_target': processed_targets,\n",
    "        'processed_method': processed_methods,\n",
    "    })\n",
    "    return processed\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'dataset' is your existing dataset variable\n",
    "processed = mask_dataset(dataset)\n",
    "\n",
    "# Check a sample processed method and target\n",
    "print(processed[\"processed_method\"][0])\n",
    "print(processed[\"processed_target\"][0])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_dataset(dataset):\n",
    "\n",
    "    processed_methods = []\n",
    "    processed_targets = []\n",
    "    i = 0\n",
    "\n",
    "    # Loop through the dataset and apply processing\n",
    "    yes = 0\n",
    "    no = 0\n",
    "    while i <= 49999:\n",
    "        # Get the current method and target block\n",
    "            if (i + 1) % 250 == 0: print(f\"Processed {i + 1}\")\n",
    "            flattened_method = dataset[\"train\"][\"cleaned_method\"][i]\n",
    "            target = dataset[\"train\"][\"target_block\"][i]\n",
    "\n",
    "        # Flatten the method by joining words with a single space\n",
    "            flattened_method = \" \".join(flattened_method.split())\n",
    "            flattened_method = re.sub(r'\\s*([=+\\-*/%<>!&|^(),:{}\\[\\].])\\s*', r'\\1', flattened_method)\n",
    "\n",
    "        # Normalize the target block\n",
    "            target = re.sub(r'\\s*([=+\\-*/%<>!&|^(),:{}\\[\\].])\\s*', r'\\1', target)\n",
    "\n",
    "        # Replace target with <MASK> in the flattened method\n",
    "            if target not in flattened_method:\n",
    "                no+=1\n",
    "            if target in flattened_method:\n",
    "                flattened_method = flattened_method.replace(target, \"<MASK>\")\n",
    "                yes+=1\n",
    "        # Append processed results\n",
    "            processed_methods.append(flattened_method)\n",
    "            processed_targets.append(target)\n",
    "            i += 1\n",
    "    print(yes)\n",
    "    print(no)\n",
    "    # Build Dataset (not DatasetDict)\n",
    "    processed = Dataset.from_dict({\n",
    "        'processed_target': processed_targets,\n",
    "        'processed_method': processed_methods,\n",
    "    })\n",
    "    return processed\n",
    "#variable for processed datatset\n",
    "processed = mask_dataset(dataset)\n",
    "#make sure dataset is masked properly \n",
    "print(processed[\"processed_method\"][0])\n",
    "print(processed[\"processed_target\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Convert to Pandas DataFrame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mprocessed\u001b[49m\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Save to CSV\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasked_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'processed' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert to Pandas DataFrame\n",
    "df = processed.to_pandas()\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"masked_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['processed_target', 'processed_method'],\n",
      "    num_rows: 50000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "processed = Dataset.from_pandas(pd.read_csv(\"masked_dataset.csv\"))\n",
    "print(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50000/50000 [00:42<00:00, 1164.15 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['processed_target', 'processed_method', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 50000\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"processed_method\"]\n",
    "    targets = examples[\"processed_target\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "\n",
    "tokenized_datasets = processed.map(preprocess_function, batched=True)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets[\"processed_method\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 5. Define Training Arguments and Trainer\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./codet5-finetuned2\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"processed_method\"],\n",
    "    eval_dataset=tokenized_datasets[\"processed_target\"],\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# 6. Train the Model\n",
    "# ------------------------\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate \n",
    "bleu = evaluate.load(\"bleu\")\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(\"SacreBLEU Score: \", results)\n",
    "language = \"python\"\n",
    "print(predictions)\n",
    "print(references)\n",
    "# Compute CodeBLEU\n",
    "#score = calc_codebleu(references, predictions, language)\n",
    "#print(\"CodeBLEU Score: \", score)\n",
    "res = calc_codebleu([[ref] for ref in references], predictions, lang=\"python\")\n",
    "print(res)\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "SacreBLEU Score:  {'bleu': 0.8265168183793802, 'precisions': [0.9166666666666666, 0.8181818181818182, 0.8, 0.7777777777777778], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 12, 'reference_length': 12}\n",
    "['def sum(a, b): return a + b']\n",
    "['def add(a, b): return a + b']\n",
    "{'codebleu': 0.8251908791628888, 'ngram_match_score': 0.6434588841607617, 'weighted_ngram_match_score': 0.6573046324907937, 'syntax_match_score': 1.0, 'dataflow_match_score': 1.0}\n",
    "Exact Match Score: 0.00\n",
    "\"\"\"\n",
    "len(predictions)\n",
    "len(references)\n",
    "exact_match_score = np.mean([ref == pred for ref, pred in zip(references, predictions)])\n",
    "print(f\"Exact Match Score: {exact_match_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
